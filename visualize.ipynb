{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import numpy as np\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "\n",
    "def visualize_sentences(sentences, weight_lists, cmap_name=\"OrRd\"):\n",
    "    \"\"\"\n",
    "    Visualize multiple sentences with token weights in a list format, each with a white background.\n",
    "\n",
    "    Args:\n",
    "    - sentences (list of list of str): A list of sentences, where each sentence is a list of tokens.\n",
    "    - weight_lists (list of list of float): A list of weight lists, one for each sentence's tokens.\n",
    "    - cmap_name (str): Name of the matplotlib colormap to use.\n",
    "\n",
    "    Returns:\n",
    "    - None: Displays the highlighted sentences.\n",
    "    \"\"\"\n",
    "    # Get colormap\n",
    "    cmap = matplotlib.colormaps[cmap_name]\n",
    "    \n",
    "    # Build HTML for each sentence\n",
    "    sentence_html_list = []\n",
    "    for tokens, weights in zip(sentences, weight_lists):\n",
    "        weights = np.array(weights)\n",
    "\n",
    "        # Normalize weights to [0, 1]\n",
    "        norm_weights = (weights - min(weights)) / (max(weights) - min(weights))\n",
    "        \n",
    "        # Convert weights to colors\n",
    "        colors = [matplotlib.colors.rgb2hex(cmap(w)) for w in norm_weights]\n",
    "        \n",
    "        # Create highlighted text for the sentence\n",
    "        highlighted_sentence = \" \".join(\n",
    "            f'<span style=\"background-color:{color}; padding:0px;  font-weight:bold; color:black;\">{token}</span>'\n",
    "            for token, color in zip(tokens, colors)\n",
    "        )\n",
    "        sentence_html_list.append(f\"<li>{highlighted_sentence}</li>\")\n",
    "    \n",
    "    # Combine all sentences into a single container with a white background\n",
    "    html_content = f\"\"\"\n",
    "    <div style='font-family:monospace; background-color:white; padding:10px;'>\n",
    "        <ul style='list-style-type:none; padding:1px; margin:1px;'>\n",
    "            {''.join(sentence_html_list)}\n",
    "        </ul>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    display(HTML(html_content))\n",
    "\n",
    "\n",
    "def visualize_tokenized_sentences(tokenized_sentences, token_weights, cmap_name=\"OrRd\"):\n",
    "    # Get colormap\n",
    "    cmap = matplotlib.cm.get_cmap(cmap_name)\n",
    "    \n",
    "    # Build HTML for each sentence\n",
    "    sentence_html_list = []\n",
    "    for tokens, weights in zip(tokenized_sentences, token_weights):\n",
    "        # Normalize weights to [0, 1]\n",
    "        weights = np.array(weights)\n",
    "        norm_weights = (weights - min(weights)) / (max(weights) - min(weights))\n",
    "        \n",
    "        # Convert weights to colors\n",
    "        colors = [matplotlib.colors.rgb2hex(cmap(w)) for w in norm_weights]\n",
    "        \n",
    "        # Reconstruct the original sentence with token highlights\n",
    "        highlighted_sentence = \"\"\n",
    "        for i, token in enumerate(tokens):\n",
    "            # Remove prefix indicators (e.g., ## or Ġ) for natural appearance\n",
    "            if \"##\" in token or \"Ġ\" in token:\n",
    "                token = \" \" + token.replace(\"Ġ\", \"\")\n",
    "            if token in [\"<s>\", \"</s>\"]:\n",
    "                token = \"\"\n",
    "            color = colors[i]\n",
    "            highlighted_sentence += f'<span style=\"background-color:{color}; color:black; padding:0px; border-radius:3px; font-weight:bold;\">{token}</span>'\n",
    "        \n",
    "        # Append the highlighted sentence to the list\n",
    "        sentence_html_list.append(f\"<li>{highlighted_sentence.strip()}</li>\")\n",
    "    \n",
    "    # Combine all sentences into a single container with a white background\n",
    "    html_content = f\"\"\"\n",
    "    <div style='font-family:monospace; background-color:white; padding:10px;'>\n",
    "        <ul style='list-style-type:none; padding:1px; margin:1px;'>\n",
    "            {''.join(sentence_html_list)}\n",
    "        </ul>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    display(HTML(html_content))\n",
    "\n",
    "\n",
    "# # Example usage\n",
    "# sentences = [\n",
    "#     [\"This\", \"is\", \"a\", \"test\", \".\"],\n",
    "#     [\"Another\", \"example\", \"sentence\", \"here\", \".\"],\n",
    "#     [\"Visualizing\", \"multiple\", \"sentences\", \"is\", \"easy\", \"!\"]\n",
    "# ]\n",
    "# weight_lists = [\n",
    "#     [0.1, 0.2, 0.5, 0.8, 0.3],\n",
    "#     [0.3, 0.6, 0.9, 0.4, 0.2],\n",
    "#     [0.5, 0.3, 0.8, 0.6, 0.7, 0.9]\n",
    "# ]\n",
    "# visualize_sentences(sentences, weight_lists)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing empty model here (0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing CustomRobertaModel: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing CustomRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CustomRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CustomRobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias', 'roberta.log_reweighting']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load the specified checkpoint /fs/nexus-scratch/milis/848K/CLAP/logs/2024_12_07-17_46_23-model_HTSAT-tiny-lr_0.01-b_96-j_1-p_fp32/checkpoints/epoch_latest.pt from users.\n",
      "Load Checkpoint...\n",
      "Loaded state dict to memory (1)\n",
      "['text_branch.log_reweighting']\n",
      "Loading state dict to model (2)\n",
      "Loaded state dict to model strictly (3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CLAP_Module(\n",
       "  (model): CLAP(\n",
       "    (audio_branch): HTSAT_Swin_Transformer(\n",
       "      (spectrogram_extractor): Spectrogram(\n",
       "        (stft): STFT(\n",
       "          (conv_real): Conv1d(1, 513, kernel_size=(1024,), stride=(480,), bias=False)\n",
       "          (conv_imag): Conv1d(1, 513, kernel_size=(1024,), stride=(480,), bias=False)\n",
       "        )\n",
       "      )\n",
       "      (logmel_extractor): LogmelFilterBank()\n",
       "      (spec_augmenter): SpecAugmentation(\n",
       "        (time_dropper): DropStripes()\n",
       "        (freq_dropper): DropStripes()\n",
       "      )\n",
       "      (bn0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (patch_embed): PatchEmbed(\n",
       "        (proj): Conv2d(1, 96, kernel_size=(4, 4), stride=(4, 4))\n",
       "        (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "      (layers): ModuleList(\n",
       "        (0): BasicLayer(\n",
       "          dim=96, input_resolution=(64, 64), depth=2\n",
       "          (blocks): ModuleList(\n",
       "            (0): SwinTransformerBlock(\n",
       "              dim=96, input_resolution=(64, 64), num_heads=4, window_size=8, shift_size=0, mlp_ratio=4.0\n",
       "              (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                dim=96, window_size=(8, 8), num_heads=4\n",
       "                (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): Identity()\n",
       "              (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (1): SwinTransformerBlock(\n",
       "              dim=96, input_resolution=(64, 64), num_heads=4, window_size=8, shift_size=4, mlp_ratio=4.0\n",
       "              (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                dim=96, window_size=(8, 8), num_heads=4\n",
       "                (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath()\n",
       "              (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=96, out_features=384, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (fc2): Linear(in_features=384, out_features=96, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (downsample): PatchMerging(\n",
       "            input_resolution=(64, 64), dim=96\n",
       "            (reduction): Linear(in_features=384, out_features=192, bias=False)\n",
       "            (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicLayer(\n",
       "          dim=192, input_resolution=(32, 32), depth=2\n",
       "          (blocks): ModuleList(\n",
       "            (0): SwinTransformerBlock(\n",
       "              dim=192, input_resolution=(32, 32), num_heads=8, window_size=8, shift_size=0, mlp_ratio=4.0\n",
       "              (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                dim=192, window_size=(8, 8), num_heads=8\n",
       "                (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath()\n",
       "              (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (1): SwinTransformerBlock(\n",
       "              dim=192, input_resolution=(32, 32), num_heads=8, window_size=8, shift_size=4, mlp_ratio=4.0\n",
       "              (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                dim=192, window_size=(8, 8), num_heads=8\n",
       "                (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath()\n",
       "              (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (downsample): PatchMerging(\n",
       "            input_resolution=(32, 32), dim=192\n",
       "            (reduction): Linear(in_features=768, out_features=384, bias=False)\n",
       "            (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (2): BasicLayer(\n",
       "          dim=384, input_resolution=(16, 16), depth=6\n",
       "          (blocks): ModuleList(\n",
       "            (0): SwinTransformerBlock(\n",
       "              dim=384, input_resolution=(16, 16), num_heads=16, window_size=8, shift_size=0, mlp_ratio=4.0\n",
       "              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                dim=384, window_size=(8, 8), num_heads=16\n",
       "                (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath()\n",
       "              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (1): SwinTransformerBlock(\n",
       "              dim=384, input_resolution=(16, 16), num_heads=16, window_size=8, shift_size=4, mlp_ratio=4.0\n",
       "              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                dim=384, window_size=(8, 8), num_heads=16\n",
       "                (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath()\n",
       "              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (2): SwinTransformerBlock(\n",
       "              dim=384, input_resolution=(16, 16), num_heads=16, window_size=8, shift_size=0, mlp_ratio=4.0\n",
       "              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                dim=384, window_size=(8, 8), num_heads=16\n",
       "                (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath()\n",
       "              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (3): SwinTransformerBlock(\n",
       "              dim=384, input_resolution=(16, 16), num_heads=16, window_size=8, shift_size=4, mlp_ratio=4.0\n",
       "              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                dim=384, window_size=(8, 8), num_heads=16\n",
       "                (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath()\n",
       "              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (4): SwinTransformerBlock(\n",
       "              dim=384, input_resolution=(16, 16), num_heads=16, window_size=8, shift_size=0, mlp_ratio=4.0\n",
       "              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                dim=384, window_size=(8, 8), num_heads=16\n",
       "                (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath()\n",
       "              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (5): SwinTransformerBlock(\n",
       "              dim=384, input_resolution=(16, 16), num_heads=16, window_size=8, shift_size=4, mlp_ratio=4.0\n",
       "              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                dim=384, window_size=(8, 8), num_heads=16\n",
       "                (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath()\n",
       "              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (downsample): PatchMerging(\n",
       "            input_resolution=(16, 16), dim=384\n",
       "            (reduction): Linear(in_features=1536, out_features=768, bias=False)\n",
       "            (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (3): BasicLayer(\n",
       "          dim=768, input_resolution=(8, 8), depth=2\n",
       "          (blocks): ModuleList(\n",
       "            (0): SwinTransformerBlock(\n",
       "              dim=768, input_resolution=(8, 8), num_heads=32, window_size=8, shift_size=0, mlp_ratio=4.0\n",
       "              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                dim=768, window_size=(8, 8), num_heads=32\n",
       "                (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath()\n",
       "              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (1): SwinTransformerBlock(\n",
       "              dim=768, input_resolution=(8, 8), num_heads=32, window_size=8, shift_size=0, mlp_ratio=4.0\n",
       "              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn): WindowAttention(\n",
       "                dim=768, window_size=(8, 8), num_heads=32\n",
       "                (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (drop_path): DropPath()\n",
       "              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (act): GELU(approximate='none')\n",
       "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (drop): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (avgpool): AdaptiveAvgPool1d(output_size=1)\n",
       "      (maxpool): AdaptiveMaxPool1d(output_size=1)\n",
       "      (tscam_conv): Conv2d(768, 527, kernel_size=(2, 3), stride=(1, 1), padding=(0, 1))\n",
       "      (head): Linear(in_features=527, out_features=527, bias=True)\n",
       "    )\n",
       "    (text_branch): CustomRobertaModel(\n",
       "      (embeddings): RobertaEmbeddings(\n",
       "        (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "        (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "        (token_type_embeddings): Embedding(1, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): CustomRobertaEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (2): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (3): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (4): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (5): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (6): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (7): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (8): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (9): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (10): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (11): RobertaLayer(\n",
       "            (attention): RobertaAttention(\n",
       "              (self): RobertaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): RobertaSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): RobertaIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): RobertaOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pooler): RobertaPooler(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "    )\n",
       "    (text_transform): MLPLayers(\n",
       "      (nonlin): ReLU()\n",
       "      (sequential): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "        (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (text_projection): Sequential(\n",
       "      (0): Linear(in_features=768, out_features=512, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "    )\n",
       "    (audio_transform): MLPLayers(\n",
       "      (nonlin): ReLU()\n",
       "      (sequential): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "        (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (audio_projection): Sequential(\n",
       "      (0): Linear(in_features=768, out_features=512, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "from src import laion_clap\n",
    "\n",
    "\n",
    "checkpoint_path = \"/fs/nexus-scratch/milis/848K/CLAP/logs/2024_12_07-17_46_23-model_HTSAT-tiny-lr_0.01-b_96-j_1-p_fp32/checkpoints/epoch_latest.pt\"\n",
    "\n",
    "# checkpoint_path = \"/fs/nexus-scratch/milis/848K/CLAP/models/630k-audioset-best.pt\"\n",
    "\n",
    "\n",
    "model = laion_clap.CLAP_Module()\n",
    "model.load_ckpt(checkpoint_path)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([-0.1206,  0.0000,  0.3762,  ...,  0.0000,  0.0000,  0.0000],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "log_reweighting = model.model.text_branch.log_reweighting\n",
    "tokenizer = model.tokenizer\n",
    "\n",
    "print(log_reweighting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    token_ids = tokenizer.encode(text)\n",
    "    tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "    return token_ids, tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'The', 'Ġsound', 'Ġof', 'Ġthe', 'Ġocean', 'Ġwaves', 'Ġcrashing', 'Ġwhile', 'Ġa', 'Ġkid', 'Ġis', 'Ġyelling', '.', '</s>']\n",
      "['<s>', 'You', 'Ġcan', 'Ġhear', 'Ġa', 'Ġlot', 'Ġof', 'Ġcar', 'Ġnoises', 'Ġin', 'Ġthe', 'Ġbackground', '.', '</s>']\n",
      "['<s>', 'A', 'Ġperson', 'Ġis', 'Ġrunning', 'Ġand', 'Ġyou', 'Ġcan', 'Ġhear', 'Ġtheir', 'Ġfoot', 'run', '.', '</s>']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div style='font-family:monospace; background-color:white; padding:10px;'>\n",
       "        <ul style='list-style-type:none; padding:1px; margin:1px;'>\n",
       "            <li><span style=\"background-color:#fedfb4; color:black; padding:0px; border-radius:3px; font-weight:bold;\"></span><span style=\"background-color:#fdd49e; color:black; padding:0px; border-radius:3px; font-weight:bold;\">The</span><span style=\"background-color:#fee9ca; color:black; padding:0px; border-radius:3px; font-weight:bold;\"> sound</span><span style=\"background-color:#fdcb95; color:black; padding:0px; border-radius:3px; font-weight:bold;\"> of</span><span style=\"background-color:#fee0b7; color:black; padding:0px; border-radius:3px; font-weight:bold;\"> the</span><span style=\"background-color:#f57850; color:black; padding:0px; border-radius:3px; font-weight:bold;\"> ocean</span><span style=\"background-color:#fc9560; color:black; padding:0px; border-radius:3px; font-weight:bold;\"> waves</span><span style=\"background-color:#f4754f; color:black; padding:0px; border-radius:3px; font-weight:bold;\"> crashing</span><span style=\"background-color:#fdd49e; color:black; padding:0px; border-radius:3px; font-weight:bold;\"> while</span><span style=\"background-color:#fee1b9; color:black; padding:0px; border-radius:3px; font-weight:bold;\"> a</span><span style=\"background-color:#fff7ec; color:black; padding:0px; border-radius:3px; font-weight:bold;\"> kid</span><span style=\"background-color:#fb8b58; color:black; padding:0px; border-radius:3px; font-weight:bold;\"> is</span><span style=\"background-color:#fc935f; color:black; padding:0px; border-radius:3px; font-weight:bold;\"> yelling</span><span style=\"background-color:#feecd1; color:black; padding:0px; border-radius:3px; font-weight:bold;\">.</span><span style=\"background-color:#7f0000; color:black; padding:0px; border-radius:3px; font-weight:bold;\"></span></li><li><span style=\"background-color:#feebd0; color:black; padding:0px; border-radius:3px; font-weight:bold;\"></span><span style=\"background-color:#fdce98; color:black; padding:0px; border-radius:3px; font-weight:bold;\">You</span><span style=\"background-color:#fdd29c; color:black; padding:0px; border-radius:3px; font-weight:bold;\"> can</span><span style=\"background-color:#fee6c4; color:black; padding:0px; border-radius:3px; font-weight:bold;\"> hear</span><span style=\"background-color:#feeed5; color:black; padding:0px; border-radius:3px; font-weight:bold;\"> a</span><span style=\"background-color:#fee1b9; color:black; padding:0px; border-radius:3px; font-weight:bold;\"> lot</span><span style=\"background-color:#fdd9aa; color:black; padding:0px; border-radius:3px; font-weight:bold;\"> of</span><span style=\"background-color:#fdc58e; color:black; padding:0px; border-radius:3px; font-weight:bold;\"> car</span><span style=\"background-color:#fdce97; color:black; padding:0px; border-radius:3px; font-weight:bold;\"> noises</span><span style=\"background-color:#fff5e7; color:black; padding:0px; border-radius:3px; font-weight:bold;\"> in</span><span style=\"background-color:#feedd3; color:black; padding:0px; border-radius:3px; font-weight:bold;\"> the</span><span style=\"background-color:#fff6ea; color:black; padding:0px; border-radius:3px; font-weight:bold;\"> background</span><span style=\"background-color:#fff7ec; color:black; padding:0px; border-radius:3px; font-weight:bold;\">.</span><span style=\"background-color:#7f0000; color:black; padding:0px; border-radius:3px; font-weight:bold;\"></span></li><li><span style=\"background-color:#fee8c9; color:black; padding:0px; border-radius:3px; font-weight:bold;\"></span><span style=\"background-color:#fff0db; color:black; padding:0px; border-radius:3px; font-weight:bold;\">A</span><span style=\"background-color:#fdb881; color:black; padding:0px; border-radius:3px; font-weight:bold;\"> person</span><span style=\"background-color:#fc9964; color:black; padding:0px; border-radius:3px; font-weight:bold;\"> is</span><span style=\"background-color:#fdd8a7; color:black; padding:0px; border-radius:3px; font-weight:bold;\"> running</span><span style=\"background-color:#fff7ec; color:black; padding:0px; border-radius:3px; font-weight:bold;\"> and</span><span style=\"background-color:#fdd49e; color:black; padding:0px; border-radius:3px; font-weight:bold;\"> you</span><span style=\"background-color:#fdce97; color:black; padding:0px; border-radius:3px; font-weight:bold;\"> can</span><span style=\"background-color:#fee2bb; color:black; padding:0px; border-radius:3px; font-weight:bold;\"> hear</span><span style=\"background-color:#fdcc96; color:black; padding:0px; border-radius:3px; font-weight:bold;\"> their</span><span style=\"background-color:#fee6c4; color:black; padding:0px; border-radius:3px; font-weight:bold;\"> foot</span><span style=\"background-color:#fdca94; color:black; padding:0px; border-radius:3px; font-weight:bold;\">run</span><span style=\"background-color:#fff4e4; color:black; padding:0px; border-radius:3px; font-weight:bold;\">.</span><span style=\"background-color:#7f0000; color:black; padding:0px; border-radius:3px; font-weight:bold;\"></span></li>\n",
       "        </ul>\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentences = [\n",
    "    \"The sound of the ocean waves crashing while a kid is yelling.\",\n",
    "    \"You can hear a lot of car noises in the background.\",\n",
    "    \"A person is running and you can hear their footrun.\"\n",
    "]\n",
    "\n",
    "token_list = []\n",
    "weight_list = []\n",
    "for sentence in sentences:\n",
    "    token_ids, tokens = tokenize(sentence)\n",
    "    print(tokens)\n",
    "    token_list.append(tokens)\n",
    "\n",
    "    log_weights = log_reweighting[token_ids]\n",
    "    weight_list.append(torch.exp(log_weights).squeeze().tolist())\n",
    "\n",
    "\n",
    "visualize_tokenized_sentences(token_list, weight_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
